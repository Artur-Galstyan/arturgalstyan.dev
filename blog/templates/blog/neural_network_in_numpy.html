{% extends "base_template.html" %}

{% block header %}
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/atom-one-light.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/highlight.min.js"></script>
<script>hljs.highlightAll();</script>
<script defer>

    function performScroll(element, duration = 1500) {
        $("html, body").animate({ scrollTop: element.offset().top }, duration);

    }

    $(window).on("load", function () {
        setTimeout(() => {
            $('#spinner').fadeOut(duration = 500)
        }, 250);
    })

    $(document).ready(function () {
        $('span code').each(function (i, inline) {
            hljs.highlightBlock(inline);
        });
    })
</script>


{% endblock %}

{% block main %}
{% include 'navbar.html' %}
{% include 'spinner.html' %}
<div class="w-[80%] md:w-[70%] lg:w-[50%] 2xl:w-[40%] mx-auto mt-16">
    <div class="text-3xl text-blue-600">
        A Neural Network Coded in Numpy
    </div>
    <div class="text-sm text-gray-500">
        15.07.2022
    </div>

    <div class="main-content text-justify my-12">
        <h1>
            Introduction
        </h1>
        A neural network (NN) is a computational model, which took inspiration from biological brains. NNs can be
        treated as universal function approximators, meaning that - given enough training data - a NN can learn any
        function (in <a href="https://www.cs.cmu.edu/~epxing/Class/10715/reading/Kornick_et_al.pdf">theory</a>). In this
        blog post, we will implement a NN using nothing but <a href="https://numpy.org">Numpy</a>. But first, we
        need to discuss the basics.

        <h1>
            Neural Network Architecture
        </h1>
        A NN is structured in 3 parts: the input layer, some hidden layers and the output layer. We represent these
        layers (and the weights and biases that connect them) using matrix notation. For example, if your training data
        consists of $m=25$ examples with each having $n=2$ <i>features</i>, the resulting matrix for your input layer
        would have the shape
        $$
        X: [25 \times 2]
        $$
        or more generally
        $$
        X: [m \times n],
        $$
        where $m$ is the number of training examples and $n$ is the number of features. A feature in your training data
        is just some characteristic that describes your data. For example, if you wanted to predict housing prices,
        possible features could be the age of the house or the living area in $m^2$.
        <br><br>
        Let's say our NN has only 1 hidden layer with 16 neurons. We denote the number of neurons in the hidden layer as
        $h_1$. Since we have $m$ training examples, we also need to have $m$ rows in our hidden layer - one for each
        training example. This means that our hidden layer $H_1$ has the shape of

        $$
        H_1: [m \times h_1],
        $$

        where $H_1$ is the reference to the first hidden layer, and $h_1$ is the number of neurons in the layer $H_1$.
        The last layer - the output layer - has $o$ output neurons. The number depends on what you are trying to predict
        or model with the NN. If you want to predict the prices of houses using their age and living area as features,
        then you would have only 1 output neuron, namely the price of the house. For classification problems where you
        have $n$ classes, you would have $n$ output neurons, which would correspond to the probability of the class. In
        general, the last layer $O$ has the shape

        $$
        O: [m \times o],
        $$
        where $o$ is the number of output neurons.

        <img src="{{ url_for('static', filename='images/blog_posts/neural_network_in_numpy/nn1.jpg') }}"
            alt="The Neural Network Architecture">

        <h1>
            Weights and Biases
        </h1>

        The layers in our NN are connected through weights and biases, which correspond to the main parameters of our
        NN. The input layer is multiplied with the weights between the input and hidden layer and the output of that
        multiplication is the hidden layer. Here, it is helpful to know the rules of matrix multiplication; here a quick
        refresher (note that the order in matrix multiplication matters):

        <img src="{{ url_for('static', filename='images/blog_posts/neural_network_in_numpy/matrix_rules.jpg') }}"
            alt="Matrix Multiplication Rules">

        With this, we can infer the shape of our weight matrix. Since we know that the hidden layer has the shape
        $H_1:[m \times h_1]$, there is only one multiplication that can result in that shape:
        $$
        X \cdot W_{X \rightarrow H_1} + b_{H_1},
        $$
        where $W_{X \rightarrow H_1}$ has the shape $n \times h_1$ and $b_{H_1}$ is the bias has the same shape as
        $H_1$.
        <img id="x_w1" src="{{ url_for('static', filename='images/blog_posts/neural_network_in_numpy/x_w1.jpg') }}"
            alt="First Matrix Multiplication">

        Similarly, the weight matrix between the hidden layer $H_1$ and the output layer $O$ has the shape
        $$
        W_{H_1 \rightarrow O}: [h_1 \times o],
        $$
        where $o$ is the number of output neurons. The matrix multiplication in that case looks similar to the one
        before:
        <img src="{{ url_for('static', filename='images/blog_posts/neural_network_in_numpy/h1_w2.jpg') }}"
            alt="Second Matrix Multiplication">

        With that we have defined the main parameters of our NN, which are the weights and biases and also how the
        matrix multiplication in a NN is performed. Let's dive into more detail next.

        <h1 id="forward-propagation">
            Forward Propagation
        </h1>
        We had already mentioned the matrix multiplication in the previous section, where the first layer is multiplied
        with the weight matrix and the bias is added on top, which in turn produces the values of the next layer. But
        there is one part missing, namely the activation function. NNs are nonlinear function approximators, which is
        due to these nonlinear activation functions. Let's write this down in math. First, let's give each of our layers
        an index:
        <img src="{{ url_for('static', filename='images/blog_posts/neural_network_in_numpy/nn_index.jpg') }}"
            alt="Neural Network Layer Indices" />

        Let $a_0 = X$, which means that $a_0$ has the shape of $m \times n$, where $m$ is the number of training
        examples and $n$ is the number of features. Now, $a_1$ refers to the activation of the hidden layer $H_1$, but
        first, let's compute what we already talked about in the previous section.

        $$
        z_1 = a_0 \cdot W_{0 \rightarrow 1} + b_{1}
        $$

        Here, $z_1$ is the result of the multiplication between the activation of the
        <span class='tooltip' data-tippy-content="Which is the <b>input</b> layer, but note that $a_0 = X$">previous
            layer</span>
        and the weights between layer 0 and 1 (the input layer and the hidden layer). The bias is added on top. Now, the
        activation of the layer $H_1$ is
        $$
        a_1 = \sigma (z_1),
        $$
        where $\sigma$ is any nonlinear activation function, such as e.g. <i><a
                href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLU</a></i> or the <i><a
                href="https://en.wikipedia.org/wiki/Sigmoid_function#:~:text=8%20External%20links-,Definition,refer%20to%20the%20same%20object.">Sigmoid</a></i>
        function:

        <div class="flex justify-center">
            <img class="w-[50%]"
                src="{{ url_for('static', filename='images/blog_posts/neural_network_in_numpy/relu.png') }}"
                alt="The ReLU Function">
            <img class="w-[50%]"
                src="{{ url_for('static', filename='images/blog_posts/neural_network_in_numpy/sigmoid.png') }}"
                alt="The Sigmoid Function">
        </div>

        <div class="text-sm text-gray-600 text-center mb-4">
            ReLU on the left and Sigmoid on the right.
        </div>

        The shape of $a_1$ is $m \times h_1$, where $h_1$ is the <span class='tooltip'
            data-tippy-content="I keep repeating this so much to really hammer in the notation that we're using">number
            of hidden neurons.</span> For our last layer $O$, we repeat the process:

        $$
        \begin{align*}

        z_2 &= a_1 \cdot W_{1 \rightarrow 2} + b_{2}\\
        a_2 &= \sigma (z_2),

        \end{align*}
        $$

        where the shape of $a_2$ is $m \times o$. If you need a refresher on the shapes of the matrices, have a look at
        the <span class="text-green-500 cursor-pointer" onclick="performScroll($('#x_w1'), 500)">matrix
            multiplications</span> again.

        <h1>Coding - Part 1: Data</h1>
        Let's code out what we have learned about so far. I assume that you have already a bit of Python experience and
        that you have <a href="https://numpy.org">Numpy</a> already installed. First, let's import Numpy and define the
        architecture of our NN.
        <pre><code>import numpy as np

# To ensure reproducibility
random_seed = 42
np.random.seed(random_seed)

neural_network = [
    {'in': 784, 'out': 16, 'activation': 'relu'},
    {'in': 16, 'out': 10, 'activation': 'sigmoid'}
]
</code></pre>
        Our network has 784 input neurons, which might confuse you. Rightfully so, because we haven't talked about our
        data yet. We will be training our NN on the <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a> dataset, which
        contains handwritten single digits. Let's write a method, which will download and preprocess the data. For that
        we need a different library called <a href="https://scikit-learn.org/stable/">sklearn</a>. The point of this
        exercise is to learn about neural networks and not necessarily about data processing. Hence, we can leverage
        existing libraries that take care of that process such that we can focus on NNs instead.
        <pre><code>from sklearn.model_selection import train_test_split
from sklearn.datasets import fetch_openml
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from joblib import Memory
            
def get_mnist(batch_size=64, random_seed=42):
    def split_into_batches(x, batch_size):
        n_batches = len(x) / batch_size
        x = np.array_split(x, n_batches)
        return np.array(x, dtype=object)

    # To cache the downloaded data
    memory = Memory("./mnist")
    fetch_openml_cached = memory.cache(fetch_openml)
    mnist = fetch_openml_cached("mnist_784")

    # Split the data into training and validation sets
    X_train, X_test, y_train, y_test = train_test_split(
        mnist.data, mnist.target, test_size=0.33, random_state=random_seed
    )

    # Normalises the data
    min_max_scaler = MinMaxScaler()

    # One-Hot encodes the targets
    one_hot_encoder = OneHotEncoder()

    # Split the training data into batches
    X_train = split_into_batches(
        min_max_scaler.fit_transform(np.array(X_train)), batch_size
    )

    #
    X_test = split_into_batches(min_max_scaler.fit_transform(np.array(X_test)), 1)

    # Turn the targets into Numpy arrays and flatten the array
    y_train = np.array(y_train).reshape(-1, 1)
    y_test = np.array(y_test).reshape(-1, 1)

    # One-Hot encode the training data and split it into batches (same as with the training data)
    one_hot_encoder.fit(y_train)
    y_train = one_hot_encoder.transform(y_train).toarray()
    y_train = split_into_batches(np.array(y_train), batch_size)

    one_hot_encoder.fit(y_test)
    y_test = one_hot_encoder.transform(y_test).toarray()
    y_test = split_into_batches(np.array(y_test), 1)

    return X_train, y_train, X_test, y_test


X_train, y_train, X_test, y_test = get_mnist()
</code></pre>
        There is a lot going on in that function. First, we download the data and cache it, such that in subsequent test
        runs, we don't have to download the data again every time. Then we perform a
        <span><code>train_test_split</code></span>, which splits the whole data into 2 chunks; in our case ~66.67% of
        the data becomes training data and the remaining ~33% become validation data. The difference between these two
        is that our NN only uses the training data for learning and the validation data is kept hidden from it. We hide
        it because we want to test the ability of the NN to <b>generalise</b> to new, never-seen before data. If the NN
        already saw the validation data during training it would effectively memorise the data and not generalise.
        <br><br>
        Next, we normalise the data, because as it was in its raw state, the values ranged from [0, 255], which is the
        "darkness" of a pixel. The higher the value, the darker the pixel. We normalise the data into the range of [0,
        1]. No information is lost here, it's just that NNs tend to be unstable when large numbers are involved. You
        can test that yourself by not performing the normalisation step and see what happens!
        <br><br>
        Now that we have the data ready, let's actually have a look at what's inside. Each handwritten digit is a
        28x28 image; hence our NN has <span class='tooltip'
            data-tippy-content='$28^2 = 784$; The 2-D (28x28) array is flattened into a 1-D (1x784) array'>784
            input
            nodes.</span>
        Here's an example of what these digits look like
        <pre><code>import matplotlib.pyplot as plt

X_train, y_train, X_test, y_test = get_mnist()
# Get from the first batch the first example and shape it back to its original form of 28x28 pixels
example = X_train[0][0].reshape(28, 28)
# Get the corresponding target value. Since its a one hot encoded array, we use argmax.
example_target = np.argmax(y_train[0][0])

plt.imshow(example)
plt.title(f"It's a {example_target}!")
plt.show()
</code></pre>
        <img class="my-4 md:w-[60%] mx-auto"
            src="{{ url_for('static', filename='images/blog_posts/neural_network_in_numpy/athree.png') }}"
            alt="It's a 3!">

        And we are going to build a NN which will be able to identify these numbers just by looking at their pixel
        values. Perhaps you are a little confused as to what "one-hot encoding" means. Initially, the data was a 28x28
        pixel image and the target was a simple string, e.g. "3". In the end we want to have a probability estimate from
        the NN, e.g. how confident is the NN that the given number is a 3. Therefore we map all possible classes to an
        array and set one value in the array to 1 (hence the name, one element in the array is "hot").
        <br><br>
        Doing that makes sense. If the given number is a 3 and we make the target look like this
        <pre><code>[0 0 0 1 0 0 0 0 0 0]</code></pre>then we essentially tell the NN to be as confident as possible
        that this particular example is a 3. Hopefully, that was understandable (and if not, you can <a
            href="mailto:mail@arturgalstyan.dev">shoot me a mail</a>).

        <h1>
            Coding - Part 2: Weights and Biases
        </h1>

        In the previous section we defined the architecture of our NN and prepared our data. Now, we need to actually
        create the weights and biases of our NN. To do that, we need to iterate over our architecture dictionary and
        initialize the weight and bias matrices.
        <pre><code>def initialise_weights_and_biases(nn_architecture):
parameters = {}
for idx, layer in enumerate(nn_architecture):
    n_input = layer["in"]
    n_output = layer["out"]

    parameters[f"weights {idx}->{idx+1}"] = np.random.randn(n_input, n_output) * 0.1
    parameters[f"bias {idx+1}"] = (
        np.random.randn(
            n_output,
        )
        * 0.1
    )

return parameters
</code></pre>
        In the NN architecture we had also defined the activation functions, so let's also code them out.
        <pre><code>def sigmoid(Z):
    return 1 / (1 + np.exp(-Z))
    
    
def relu(Z):
    return np.maximum(0, Z)
</code></pre>
        With this, everything should be ready for coding out the forward propagation of the NN as <span
            class="text-green-500 cursor-pointer" onclick="performScroll($('#forward-propagation'), 500)">discussed
            earlier</span>.
    </div>

</div>



{% endblock %}