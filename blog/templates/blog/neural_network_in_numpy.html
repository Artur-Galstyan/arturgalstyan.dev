{% extends "base_template.html" %}

{% block header %}
<script>
    MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']]
        }
    };
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script defer>

    $(window).on("load", function () {
        setTimeout(() => {
            $('#spinner').fadeOut(duration = 500)
        }, 250);
    })
</script>


{% endblock %}

{% block main %}
{% include 'navbar.html' %}
{% include 'spinner.html' %}
<div class="w-[80%] md:w-[70%] lg:w-[50%] 2xl:w-[30%] mx-auto mt-16">
    <div class="text-3xl text-blue-600">
        A Neural Network Coded in Numpy
    </div>
    <div class="text-sm text-gray-500">
        15.07.2022
    </div>

    <div class="main-content text-justify my-12">
        <h1>
            Introduction
        </h1>
        A neural network (NN) is a computational model, which took inspiration from biological brains. NNs can be
        treated as universal function approximators, meaning that - given enough training data - a NN can learn any
        function (in <a href="https://www.cs.cmu.edu/~epxing/Class/10715/reading/Kornick_et_al.pdf">theory</a>). In this
        blog post, we will implement a NN using nothing but <a href="https://numpy.org">Numpy</a>. But first, we
        need to discuss the basics.

        <h1>
            Neural Network Architecture
        </h1>
        A NN is structured in 3 parts: the input layer, some hidden layers and the output layer. We represent these
        layers (and the weights and biases that connect them) using matrix notation. For example, if your training data
        consists of $m=25$ examples with each having $n=2$ <i>features</i>, the resulting matrix for your input layer
        would have the shape
        $$
        X: [25 \times 2]
        $$
        or more generally
        $$
        X: [m \times n],
        $$
        where $m$ is the number of training examples and $n$ is the number of features. A feature in your training data
        is just some characteristic that describes your data. For example, if you wanted to predict housing prices,
        possible features could be the age of the house or the living area in $m^2$.
        <br><br>
        Let's say our NN has only 1 hidden layer with 16 neurons. We denote the number of neurons in the hidden layer as
        $h_1$. Since we have $m$ training examples, we also need to have $m$ rows in our hidden layer - one for each
        training example. This means that our hidden layer $H_1$ has the shape of

        $$
        H_1: [m \times h_1],
        $$

        where $H_1$ is the reference to the first hidden layer, and $h_1$ is the number of neurons in the layer $H_1$.
        The last layer - the output layer - has $o$ output neurons. The number depends on what you are trying to predict
        or model with the NN. If you want to predict the prices of houses using their age and living area as features,
        then you would have only 1 output neuron, namely the price of the house. For classification problems where you
        have $n$ classes, you would have $n$ output neurons, which would correspond to the probability of the class. In
        general, the last layer $O$ has the shape

        $$
        O: [m \times o],
        $$
        where $o$ is the number of output neurons.

        <img src="{{ url_for('static', filename='images/blog_posts/neural_network_in_numpy/nn1.jpg') }}"
            alt="The Neural Network Architecture">

        <h1>
            Weights and Biases
        </h1>

        The layers in our NN are connected through weights and biases, which correspond to the main parameters of our
        NN. The input layer is multiplied with the weights between the input and hidden layer and the output of that
        multiplication is the hidden layer. Here, it is helpful to know the rules of matrix multiplication; here a quick
        refresher (note that the order in matrix multiplication matters):

        <img src="{{ url_for('static', filename='images/blog_posts/neural_network_in_numpy/matrix_rules.jpg') }}"
            alt="Matrix Multiplication Rules">

        With this, we can infer the shape of our weight matrix. Since we know that the hidden layer has the shape
        $H_1:[m \times h_1]$, there is only one multiplication that can result in that shape:
        $$
        X \cdot W_{X \rightarrow H_1} + b_{H_1},
        $$
        where $W_{X \rightarrow H_1}$ has the shape $n \times h_1$ and $b_{H_1}$ is the bias has the same shape as
        $H_1$.
        <img src="{{ url_for('static', filename='images/blog_posts/neural_network_in_numpy/x_w1.jpg') }}"
            alt="First Matrix Multiplication">

        Similarly, the weight matrix between the hidden layer $H_1$ and the output layer $O$ has the shape
        $$
        W_{H_1 \rightarrow O}: [h_1 \times o],
        $$
        where $o$ is the number of output neurons. The matrix multiplication in that case looks similar to the one
        before:
        <img src="{{ url_for('static', filename='images/blog_posts/neural_network_in_numpy/h1_w2.jpg') }}"
            alt="Second Matrix Multiplication">

        With that we have defined the main parameters of our NN, which are the weights and biases and also how the
        matrix multiplication in a NN is performed. Let's dive into more detail next.

        <h1>
            Forward Propagation
        </h1>
        We had already mentioned the matrix multiplication in the previous section, where the first layer is multiplied
        with the weight matrix and the bias is added on top, which in turn produces the values of the next layer. But
        there is one part missing, namely the activation function. NNs are nonlinear function approximators, which is
        due to these nonlinear activation functions. Let's write this down in math. First, let's give each of our layers
        an index:
        <img src="{{ url_for('static', filename='images/blog_posts/neural_network_in_numpy/nn_index.jpg') }}"
            alt="Neural Network Layer Indices" />

        Let $a_0 = X$, which means that $a_0$ has the shape of $m \times n$, where $m$ is the number of training
        examples and $n$ is the number of features. Now, $a_1$ refers to the activation of the hidden layer $H_1$, but
        first, let's compute what we already talked about in the previous section.

        $$
        z_1 = a_0 \cdot W_{0 \rightarrow 1} + b_{1}
        $$

        Here, $z_1$ is the result of the multiplication between the activation of the
        <span class='tooltip' data-tippy-content="Which is the <b>input</b> layer, but note that $a_0 = X$">previous
            layer</span>
        and the weights between layer 0 and 1 (the input layer and the hidden layer). The bias is added on top.





    </div>

</div>



{% endblock %}